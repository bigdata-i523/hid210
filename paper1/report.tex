\documentclass[sigconf]{acmart}

\usepackage{hyperref}

\usepackage{endfloat}
\renewcommand{\efloatseparator}{\mbox{}} % no new page between figures

\usepackage{booktabs} % For formal tables

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\begin{document}
\title{Automated Information Extraction in Electronic Health Records}

\author{Nicholas J Hotz}
\affiliation{%
  \institution{Indiana University}
}
\email{nhotz@iu.edu}

\begin{abstract}
Electronic medical records (EMRs) play an increasingly important role in health care. However, the rapidly growing volume of text in EMRs creates challenges in the extraction of information. As such, many research institutions are developing computer-based systems to automate EMR structured information extraction (IE). This paper investigates the processes, the challenges, and the current state of automated IE of EMRs with a specific focus on automated systems that comprehensively extract ICD9 codes from clinical text. While automated system performance has caught up to the accuracy of manual coding under specific circumstances, automated code extraction remains mostly an academic exercise. However, recall seems sufficient for commercial recommendation systems to support manual coders and for audit purposes. 
\end{abstract}

\keywords{Natural Language Processing, Information Extraction, Clinical Coding, Electronic Health Records}


\maketitle

\section{Introduction}
Demand for structured health data  continues to grow \cite{stanfill2010systematic}, and the adoption of electronic health records (EMRs) generates new opportunities to improve clinical care, administrative processes, clinical workflows, and patient outcomes through higher quality, more accurate, more consistent, and more easily accessible documentation \cite{meystre2008extracting}\cite{pons2016natural}. However, EMRs also create challenges, in part because EMR information is often stored in narrative form which describe patients, their own and their family's medical history, their personal lifestyle, and their current medical conditions. \cite{meystre2008extracting} Although convenient for documentation, narrative text is difficult for computer systems to interpret as coded data that can support research, provide clinical knowledge and performance information, and improve patient outcomes \cite{stanfill2010systematic}\cite{meystre2008extracting}.

Commonly studied clinical NLP problems include de-identification \cite{velupillai2015recent}, the development of patient problem summaries \cite{diomaiuta2017novel}, and diagnostic code extraction \cite{perotte2013diagnosis}. This paper focuses on diagnostic code extraction which is the process of converting EMR clinical narratives into appropriate medical codes such as ICD9 (the standard medical diagnostic hierarchical taxonomy system in the United States until September 30, 2015). Perotte et al. describe that both the ICD9 and the more recently adopted ICD10 taxonomies as "organized in a rooted tree structure, with edges representing is-a relationships between parents and children" \cite{perotte2013diagnosis}. According to Kavauluru et al, leaf nodes are codes that provide specific information used for "billing and reimbursement, quality control, epidemiological studies, and cohort identification for clinical trials" \cite{kavuluru2015empirical}.

Currently, coding professionals and physicians manually extract diagnostic codes from EMRs which is expensive, inefficient, and has become increasingly complex due to various factors including the expansion of payment systems, new reporting requirements, increased oversight and regulation, and the increased volume of EMR data \cite{stanfill2010systematic} \cite{AHIMA} \cite{pons2016natural}\cite{velupillai2015recent}. This complexity limits manual coding accuracy. Manual coders often disagree \cite{pestian2007shared} and are more specific than sensitive in their code assignments \cite{birman2005accuracy}. Errors are prevalent; for example a Sweedish study of 4,200 patient records found errors in 20\% of the main diagnoses \cite{velupillai2015recent}. Over-coding can be viewed as fraudulent because health care providers would bill for services not rendered while under-coding prevents providers from earning reimbursements for valid conditions and services \cite{perotte2013diagnosis}.

Since the 1990s \cite{kavuluru2013unsupervised}, researchers have tried to improve the coding processes through automated coding and classification technologies which, according to Stanfill et al, "encompass a variety of computer-based approaches that transform narrative text in clinical records into structured text, which may include assignment of codes from standard terminologies, without human interaction"\cite{stanfill2010systematic}. In 2004, the American Health Information Management Association asserted that "The industry needs automated solutions to allow the coding process to become more productive, efficient, accurate, and consistent‚Äù \cite{stanfill2010systematic}. However, Stanfill et al. conclude in their 2010 literature review that the relative performance of automated systems to manual coding is not yet known\cite{stanfill2010systematic}. As of 2008 and still in 2015, automated systems are still mostly used for research purposes with few applications in use by practitioners \cite{meystre2008extracting}\cite{velupillai2015recent}. 



\section{EMR Information Extraction Challenges}
Several challenges have slowed the development of clinical text NLP applications, which lag behind NLP applications in other fields\cite{chapman2011overcoming}. Meystre, et al attribute the lack of shareable clinical data as the biggest challenge \cite{meystre2008extracting}. Large annotated corpora are needed to develop effective machine learning algorithms that can classify roughly 17,000 possible ICD-9 codes and 68,000 ICD-10 codes whose frequency distributions are highly skewed \cite{berndorfer2017automated}. However, clinical information needs to be de-identified (which itself is a challenging problem) in order to comply with privacy concerns and regulations such the USA's Health Information Portability and Protection Act (HIPAA) and the European Union's General Data Protection Regulation (GDPR); consequently large corpora are rarely available from other health systems \cite{meystre2008extracting}\cite{stanfill2010systematic}.

As a related problem, even when corpora are available, the annotation process is time-consuming, expensive, and traditionally relies on domain experts and linguists  \cite{meystre2008extracting}\cite{velupillai2015recent}. Given the highly specific sublanguages of clinical text, general NLP systems perform poorly on cross-domain clinical texts without these comprehensive annotated corpora. Consequently, much of the development in clinical text NLP occur in siloes and are not used outside of the laboratory in which they were developed \cite{chapman2011overcoming}.

In addition to the lack of shared annotated corpora, Meystre et al. present four challenges that hinder the development of effective clinical text IE. First, clinical narratives contain ungrammatical phrases with short-hand abbreviations and acronyms. About a third of these short-hand texts are overloaded (a single unit may have multiple meanings) which can be challenging for human interpretation and even more challenging for computer interpretation. Second, the rate of misspellings is around 10 \% \cite{ruch2003using}, which is higher than most texts and complicates several NLP techniques. Third, clinical texts often contain long series of non-text information, such as laboratory test results, which makes sentence segmentation difficult. Forth, institution-specific pre-formatted templates that appear in clinical texts are difficult for interpretation and their meanings do not transfer to other institutions' information \cite{meystre2008extracting}. Chapman et al. discuss additional challenges including the inadequacy of de-identification algorithms, the lack of focus for NLP in non-English clinical texts, and the absence of common clinical standards \cite{chapman2011overcoming}.

Fortunately, recent progress is promising as explained in literate reviews by Delanis et al (2014) and Velupillai et al (2015). These publications praise the clinical NLP community for overcoming many of these hurdles by providing more annotated corpora, developing more advanced NLP tools specific to clincal text, leveraging partially-automated processes to facilitate the annotation of corpora, and focusing on multiple languages \cite{dalianis2014didactic} \cite{velupillai2015recent}.



\section{EMR Information Extraction Pre-Processing}
To convert text to medical codes, clinical text flows through various pre-processing and context feature detection techniques. General pre-processing NLP tools are being adopted for medical texts including:
\begin{itemize}
\item \textbf{Language Detection:} Multi-lingual studies may start with language detection algorithms, although some might still rely on manual detection \cite{diomaiuta2017novel}.
\item \textbf{Spell checking:} Clinical NLP spell checking uses standard dictionaries and medical-specific tools such as unified medical language system (UMLS) and WordNet \cite{meystre2008extracting}.
\item \textbf{Word sense disambiguation:} WSD allows the system to identify the correct meaning of a word that has multiple definitions; however this process is not as accurate with clinical texts as with general English (about 90\% for general English and 80\% for clinical text) \cite{meystre2008extracting}.
\item \textbf{Tokenization and sentence-splitting:}  Tomanek et al. find that the training corpus is not too important for sentence-splitting but is crucial for tokenization, the process for breaking text into tokens such as words, phrases, or symbols \cite{tomanek2007sentence}\cite{diomaiuta2017novel}.
\item \textbf{Part-of-speech tagging:} Also known as lexical analysis, POS tagging identifies a word's part of speech and its relationship with other words in a sentence \cite{meystre2008extracting}\cite{diomaiuta2017novel}.
\item \textbf{Parsers:} Parsers identify the sentence syntax, word dependencies, and expressions of interest \cite{meystre2008extracting}\cite{diomaiuta2017novel}.
\end{itemize}

Context feature detection and analysis typically follows the above steps and identifies how words and concepts are being in the context of the sentence. Clinical NLP systems often leverage a set of regular expressions and algorithms such as NegEx, NegExpander, TimeText, and ConText to define feature context. Notable contexts are negation (e.g. patient does not have a condition), speculative (e.g. patient might have a condition) temporality (e.g. to identify if the patient currently has the condition or if the text references their medical history), subject identification (e.g. to identify if the condition belongs to the patient or some one else such as a family member), and severity (such as mild, moderate, or severe conditions) \cite{meystre2008extracting}\cite{velupillai2015recent}.



\section{Effectiveness of Automated ICD9 Code Extraction Overview}
To evaluate the effectiveness of automated systems, studies compare evaluation metrics against standards. Per Stanfill et al.'s literature review of 113 studies, 43\% of studies use the gold standard comparison which uses two or more independent reviewers with an adjudication process for disagreements, and 51\% use the regular practice standard of one reviewer \cite{stanfill2010systematic}. Although considered more reliable, gold standards are still prone to error \cite{perotte2013diagnosis}. The most commonly reported metrics include recall or sensitivity (69\%), PPV or precision (46\%), specificity (43\%), and accuracy (25\%) \cite{stanfill2010systematic}.

Most studies foucs only on a specific subset of clinical texts or diagnoses such as subdomains like radiology \cite{pons2016natural}, for specific diagnoses like congestive heart failure \cite{friedlin2006natural} or cancer \cite{mamlin2003automated}, or to extract only attributes of patients like smoking status \cite{uzuner2008identifying}. Although many of these studies achieve accuracy metrics comparable or even exceeding gold standards, their results are not generalizable for more comprehensive or practical purposes in the field \cite{stanfill2010systematic}.

However, two recent studies attempt to comprehensively extract ICD9 codes from large EMR sets. In 2013, Perotte et al. attempted to extract ICD9 codes from the clinical text of Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC II), a publicly available database containing de-identified records of 40,000 ICU hospital admissions. They split the 22,815 discharge summaries, which contain 215,826 ICD9 codes (5030 distinct) into 20,533 training documents and 2,282 testing documents. Using a hierarchy support vector machine (SVM) classifier, they achieved an F-measure of 39.5\% with a 30.0\% recall and 57.7\% precision. They also attempted a flat SVM which returned an 27.6\% F-measure with 16.4\% recall but with a higher precision (86.7\%) \cite{perotte2013diagnosis}.

In 2015, Kavuluru et al. developed automated coding systems with 71,463 in-patient EMRs from the University of Kentucky Medical Center. They conclude that the best-performing automated coding method depends on the size and characteristics of the dataset. For smaller narratives in sumdomains such as radiology or pathology, chain classifiers perform best because codes are highly related to each other. However, feature and data selection methods perform best with more comprehensive in-patient EMRs. Meanwhile, "for large EMR datasets, the binary relevance approach with learning-to-rank based code reranking offers the best performance". They reported a micro F score of 0.48 with codes that occur at last 50 times and a score of 0.54 for codes that occur in at least 1\% of records \cite{kavuluru2015empirical}.

\section{Outlook}
Add in stuff. No citations allowed. Argue for more practical-focused work.

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\end{document}
