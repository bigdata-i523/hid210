\documentclass[sigconf]{acmart}

\usepackage{hyperref}

\usepackage{endfloat}
\renewcommand{\efloatseparator}{\mbox{}} % no new page between figures

\usepackage{booktabs} % For formal tables

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\begin{document}
\title{Automated Diagnostic Code Extraction in Electronic Medical Records}

\author{Nicholas J Hotz}
\affiliation{%
  \institution{Indiana University}
}
\email{nhotz@iu.edu}

\begin{abstract}
Electronic medical records (EMRs) play an increasingly important role in healthcare. However, the rapidly growing volume of text in EMRs creates challenges for information extraction (IE). As such, many research institutions are developing computer-based systems to automate EMR structured IE. This paper investigates the processes, the challenges, and the current state of automated IE of EMRs with a specific focus on automated systems that extract ICD9 codes from clinical text. While automated system performance has caught up to the accuracy of manual coding under specific circumstances, automated code extraction remains mostly an academic exercise. To extract value from their work, researchers should shift their focus away from highly specialized algorithms that work in isolation and instead collaborate with industry to develop augmented intelligence systems that help make coding professionals more effective.
\end{abstract}

\keywords{i523, HID210, Natural Language Processing, Medical NLP, Clinical NLP, Information Extraction, Clinical Coding, Healthcare Big Data}


\maketitle

\section{Introduction}
Demand for structured health data continues to grow \cite{stanfill2010systematic}, and the adoption of electronic health records (EMRs) generates new opportunities to improve clinical care, administrative processes, clinical workflows, and patient outcomes through higher quality, more accurate, more consistent, and more easily accessible documentation \cite{meystre2008extracting} \cite{pons2016natural}.

However, the size, growth, and textual nature of EMRs render traditional software and hardware unable to effectively manage healthcare big data \cite{raghupathi2014big}. Healthcare data in the United States reached 150 exabytes in 2011 with Kaiser Permanente, Califronia's health network, reportedly having between 26.5 and 44 petabytes alone \cite{cottle2013transforming}. The volume of healthcare data is doubling every 12-14 months \cite{dinov2016volume}, and the diversity of this data further complicates its analysis \cite{frost2015drowning}. Much of it is stored in narrative form which describe patients, their own and their family's medical history, their personal lifestyle, and their current medical conditions \cite{meystre2008extracting}. Although convenient for documentation, narrative text is difficult for computer systems to interpret as coded data that can support research, provide clinical knowledge and performance information, and improve patient outcomes \cite{meystre2008extracting} \cite{stanfill2010systematic}.

Commonly studied clinical NLP problems include de-identification \cite{velupillai2015recent}, the development of patient problem summaries \cite{diomaiuta2017novel}, and diagnostic code extraction \cite{perotte2013diagnosis}. This paper focuses on diagnostic code extraction which is the process of converting EMR clinical narratives into appropriate medical codes such as ICD9 (the standard medical diagnostic hierarchical taxonomy system in the United States until September 30, 2015). Perotte et al. describe both the ICD9 and the more recently adopted ICD10 taxonomies as ``organized in a rooted tree structure, with edges representing is-a relationships between parents and children'' \cite{perotte2013diagnosis}. Kavauluru et al. explain that the ICD9 and ICD10 leaf nodes are codes that provide specific information used for ``billing and reimbursement, quality control, epidemiological studies, and cohort identification for clinical trials'' \cite{kavuluru2015empirical}.

Currently, coding professionals and clinicians manually extract diagnostic codes from EMRs which is expensive, inefficient, and has become increasingly complex due to various factors including the expansion of payment systems, new reporting requirements, increased oversight and regulation, and the increased volume of EMR data \cite{AHIMA} \cite{pons2016natural} \cite{stanfill2010systematic} \cite{velupillai2015recent}. This complexity limits manual coding accuracy. Manual coders often disagree \cite{pestian2007shared} and are more specific than sensitive in their code assignments \cite{birman2005accuracy}. Errors are prevalent; for example, a Swedish study of 4,200 patient records found errors in 20\% of the main diagnoses \cite{velupillai2015recent}. Over-coding can lead to fraud if healthcare providers bill for services not rendered while under-coding prevents providers from earning reimbursements for valid conditions and services \cite{perotte2013diagnosis}.

Since the 1990s, researchers have tried to improve the coding processes through automated coding and classification technologies \cite{kavuluru2013unsupervised}. Stanfill et al. in their comprehensive literature review in 2010 describe these automated coding systems as ``a variety of computer-based approaches that transform narrative text in clinical records into structured text, which may include assignment of codes from standard terminologies, without human interaction.'' They cite that the American Health Information Management Association asserted in 2004 that, ``The industry needs automated solutions to allow the coding process to become more productive, efficient, accurate, and consistent.'' Yet, Stanfill et al. conclude that the relative performance of automated systems to manual coding is not yet known \cite{stanfill2010systematic}. As of 2008 and still in 2015, automated systems are still mostly used for research purposes with few applications in use by practitioners \cite{meystre2008extracting} \cite{velupillai2015recent}. 



\section{EMR Information Extraction Challenges}
Several challenges have slowed the development of clinical text NLP applications, which lag behind NLP applications in other fields \cite{chapman2011overcoming}. Meystre, et al. attribute the lack of shareable clinical data as the biggest challenge \cite{meystre2008extracting}. Large annotated corpora are needed to develop effective machine learning algorithms that can classify roughly 17,000 possible ICD-9 codes and 68,000 ICD-10 codes whose frequency distributions are highly skewed \cite{berndorfer2017automated}. However, clinical information needs to be de-identified (which itself is a challenging problem) in order to comply with privacy concerns and regulations such the USA's Health Insurance Portability and Protection Act (HIPAA) and the European Union's General Data Protection Regulation (GDPR); as a consequence, large corpora typically remain siloed within individual healthcare systems and are rarely available for outsiders \cite{meystre2008extracting} \cite{stanfill2010systematic}.

As a related problem, even when corpora are available, the annotation process is time-consuming, expensive, and traditionally relies on domain experts and linguists \cite{meystre2008extracting} \cite{velupillai2015recent}. Given the highly specific sublanguages of clinical text, general NLP systems perform poorly on cross-domain clinical texts without these comprehensive annotated corpora. Consequently, much of the development in clinical text NLP occur in siloes and is not used outside of the laboratory in which they were developed \cite{chapman2011overcoming}.

In addition to the lack of shared annotated corpora, Meystre et al. present four challenges that hinder the development of effective clinical text IE. First, clinical narratives contain ungrammatical phrases with short-hand abbreviations and acronyms. About a third of these short-hand texts are overloaded (a single unit may have multiple meanings) which can be challenging for human interpretation and even more challenging for computer interpretation. Second, the rate of misspellings, around 10\% \cite{ruch2003using}, is higher than most texts complicates many NLP techniques. Third, clinical texts often contain long series of non-text information, such as laboratory test results, which makes sentence segmentation difficult. Forth, institution-specific pre-formatted templates that appear in clinical texts are difficult for interpretation and their meanings do not transfer to other institutions' information \cite{meystre2008extracting}. Chapman et al. discuss additional challenges including the inadequacy of de-identification algorithms, the lack of focus for NLP in non-English clinical texts, and the absence of common clinical standards \cite{chapman2011overcoming}.

Fortunately, recent progress is promising as explained in literature reviews by Delanis et al. (2014) and Velupillai et al. (2015). These publications praise the clinical NLP community for overcoming many of these hurdles by providing more annotated corpora, developing more advanced NLP tools specific to clinical text, leveraging partially-automated processes to facilitate the annotation of corpora, and focusing on multiple languages \cite{dalianis2014didactic} \cite{velupillai2015recent}.



\section{EMR Pre-Processing}
To convert text to medical codes, clinical text flows through various pre-processing and context feature detection techniques. General pre-processing NLP tools are being adopted and specialized for medical texts including:
\begin{itemize}
\item \textbf{Language detection:} Multi-lingual studies may start with language detection algorithms, although some might still rely on manual detection \cite{diomaiuta2017novel}.
\item \textbf{Spell checking:} Clinical NLP spell checking uses standard dictionaries and medical-specific tools such as unified medical language system (UMLS) and WordNet \cite{meystre2008extracting}.
\item \textbf{Word sense disambiguation:} WSD allows the system to identify the correct meaning of a word that has multiple definitions; however this process is not as accurate with clinical texts as with general English (about 90\% for general English and 80\% for clinical text) \cite{meystre2008extracting}.
\item \textbf{Tokenization and sentence-splitting:}  Tokenization splits text into smaller components called tokens which include words, phrases, and symbols \cite{diomaiuta2017novel} \cite{tomanek2007sentence}.
\item \textbf{Part-of-speech tagging:} Also known as lexical analysis, POS tagging identifies a word's part of speech and its relationship with other words in a sentence \cite{diomaiuta2017novel} \cite{meystre2008extracting}.
\item \textbf{Parsers:} Parsers identify the sentence syntax, word dependencies, and expressions of interest \cite{diomaiuta2017novel} \cite{meystre2008extracting}.
\end{itemize}

Context feature detection and analysis happen concurrently or following the above steps and identify how words and concepts are used in the context of the sentence. Clinical NLP systems often use a set of regular expressions and algorithms such as NegEx, NegExpander, TimeText, and ConText to define feature context. Notable contexts are negation (e.g. patient \textit{does not} have a condition), speculative (e.g. patient \textit{might have} a condition) temporality (e.g. to identify if the patient \textit{has} or \textit{had} a condition), subject identification (e.g. to identify if the condition belongs to the patient or someone else such as a family member), and severity (such as mild, moderate, or severe conditions) \cite{meystre2008extracting} \cite{velupillai2015recent}.



\section{Review of Automated ICD9 Code Extraction Effectiveness }
To evaluate the effectiveness of automated systems, studies compare evaluation metrics against standards. Per Stanfill et al.'s literature review of 113 studies, 43\% of studies use the gold standard comparison which uses at least two independent reviewers and an adjudication process to resolve inconsistencies, and 51\% use the regular practice standard of one reviewer \cite{stanfill2010systematic}. Although considered more reliable, gold standards are still prone to error \cite{perotte2013diagnosis}. The most commonly reported metrics include recall or sensitivity (69\%), PPV or precision (46\%), specificity (43\%), and accuracy (25\%) \cite{stanfill2010systematic}.

Most studies focus only on a specific subset of clinical texts or diagnoses such as subdomains like radiology \cite{pons2016natural}, for specific diagnoses like congestive heart failure \cite{friedlin2006natural} or cancer \cite{mamlin2003automated}, or to extract only attributes of patients like smoking status \cite{uzuner2008identifying}. Although many of these studies achieve accuracy metrics comparable or even exceeding gold standards, their results are not generalizable for more comprehensive or practical purposes in the field \cite{stanfill2010systematic}.

However, two recent studies attempt to comprehensively extract ICD9 codes from large EMR sets. In 2013, Perotte et al. attempted to extract ICD9 codes from the clinical text of Multiparameter Intelligent Monitoring in Intensive Care II (MIMIC II), a publicly available database containing de-identified records of 40,000 ICU hospital admissions. They split the 22,815 discharge summaries, which contain 215,826 ICD9 codes (5030 distinct) into 20,533 training documents and 2,282 testing documents. Using a hierarchy support vector machine (SVM) classifier, Perotte et al. report an F-measure of 39.5\% with a 30.0\% recall and 57.7\% precision. They also attempted a flat SVM which returned a 27.6\% F-measure with 16.4\% recall but with a higher precision (86.7\%) \cite{perotte2013diagnosis}.

Similarly, in 2015 Kavuluru et al. developed automated coding systems with 71,463 in-patient EMRs from the University of Kentucky Medical Center. They conclude that the best-performing automated coding method depends on the size and characteristics of the dataset. For smaller narratives in subdomains such as radiology or pathology, chain classifiers perform best because codes are similar to each other. However, feature and data selection methods perform best with more comprehensive in-patient EMRs. Meanwhile, ``for large EMR datasets, the binary relevance approach with learning-to-rank based code reranking offers the best performance.'' They reported a micro F score of 0.48 with codes that occur at last 50 times and a score of 0.54 for codes that occur in at least 1\% of records \cite{kavuluru2015empirical}.

\section{Conclusion}
Researchers are increasingly studying clinical NLP and diagnostic code extraction. However, the output of most research is limited to specific circumstances and has not yet been applied to practical use cases that improve the accuracy and efficiency of medical coding processes. Rather, the research community seems to evaluate its work in terms of algorithm accuracy metrics in their specific strength zones relative to the performance of human coders. Cross-domain medical coding studies are a step in the right direction toward a more practical approach which begins to mimic the reality faced by human coders.

However, the clinical NLP researchers should take this progress further, and collaborate with software engineers, HCI design specialists, business analysts, medical coders, and clinicians to develop practical augmented intelligence systems. These systems, which can include semi-automated recommendation and auditing support software solutions, can aid medical coding professionals in actual workflows to extract diagnostic codes from medical text. A workflow that leverages the strengths of algorithmic systems to shore up areas of human coder weaknesses can optimize medical coding efficiency and accuracy.

\begin{acks}
The author would like to thank Dr. Gregor von Laszewski and Juliette Zerick for their support and suggestions to write this paper.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\end{document}
